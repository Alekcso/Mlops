# -*- coding: utf-8 -*-
"""preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WNAJ3772vXT7o33VtZwYoPLC8ayRrv6g
"""

# src/preprocessing.py

import os
import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


# --- NLTK инициализация ---

def init_nltk():
    """
    Скачиваем нужные ресурсы NLTK (вызывается один раз перед обучением).
    """
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('omw-1.4')


# src/preprocessing.py

import os
import re
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# --- Глобальные переменные, которые инициализируем лениво ---
_stop_words = None
_lemmatizer = None


def init_nltk():
    """
    Скачиваем нужные ресурсы NLTK (если их ещё нет)
    и инициализируем стоп-слова и лемматизатор.
    """
    global _stop_words, _lemmatizer

    # Если уже инициализированы — ничего не делаем
    if _stop_words is not None and _lemmatizer is not None:
        return

    # Скачиваем ресурсы (если не скачаны)
    nltk.download('stopwords', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)

    _stop_words = set(stopwords.words('english'))
    _lemmatizer = WordNetLemmatizer()


def clean_text(text: str) -> str:
    """
    Полная очистка текста:
    - lower
    - удалить URL
    - удалить HTML-теги
    - оставить только [a-z и пробелы]
    - убрать стоп-слова
    - лемматизация
    """
    # гарантируем, что NLTK инициализирован
    init_nltk()

    text = str(text).lower()

    # URL
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # HTML
    text = re.sub(r'<.*?>', '', text)

    # всё, что не буквы и пробел
    text = re.sub(r'[^a-z\s]', '', text)

    tokens = text.split()
    cleaned_tokens = [
        _lemmatizer.lemmatize(word)
        for word in tokens
        if word not in _stop_words
    ]

    return " ".join(cleaned_tokens)


def make_binary_dataset(
    raw_csv_path: str,
    out_binary_path: str
) -> pd.DataFrame:
    """
    Создаём бинарную разметку sentiment и сохраняем reviews_binary.csv.

    Шаги:
    - читаем netflix_reviews.csv
    - создаём sentiment: score<=2 → negative, иначе positive
    - дропаем NaN по content
    - дропаем дубликаты по content
    - приводим content к lower
    - добавляем review_length
    """
    df = pd.read_csv(raw_csv_path)

    # целевая переменная
    df['sentiment'] = df['score'].apply(
        lambda x: 'negative' if x <= 2 else 'positive'
    )

    # чистим NaN и дубликаты
    df = df.dropna(subset=['content'])
    df = df.drop_duplicates(subset=['content'])

    # lower
    df['content'] = df['content'].str.lower()

    # длина отзыва (в словах)
    df['review_length'] = df['content'].apply(
        lambda x: len(str(x).split())
    )

    # создаём каталог, если нужно
    dirname = os.path.dirname(out_binary_path)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    df.to_csv(out_binary_path, index=False)

    return df


def make_clean_dataset(
    binary_csv_path: str,
    out_clean_path: str
) -> pd.DataFrame:
    """
    Загружаем reviews_binary.csv, считаем clean_text, выкидываем пустые,
    сохраняем reviews_binary_clean.csv.
    """
    df = pd.read_csv(binary_csv_path)

    df['clean_text'] = df['content'].apply(clean_text)
    df['clean_length'] = df['clean_text'].apply(
        lambda x: len(str(x).split())
    )

    # удалить пустые после очистки
    df = df[df['clean_length'] > 0]

    dirname = os.path.dirname(out_clean_path)
    if dirname:
        os.makedirs(dirname, exist_ok=True)

    df.to_csv(out_clean_path, index=False)

    return df

def make_3class_dataset(
    clean_csv_path: str,
    out_3class_path: str
) -> pd.DataFrame:
    """
    Создаём трёхклассовую разметку sentiment_3class (1 → neg, 2–4 → neutral, 5 → pos)
    из уже очищенных данных.
    """
    df = pd.read_csv(clean_csv_path)

    def map_3class(score):
        if score == 1:
            return 'negative'
        elif score in [2, 3, 4]:
            return 'neutral'
        else:
            return 'positive'

    df['sentiment_3class'] = df['score'].apply(map_3class)
    df.to_csv(out_3class_path, index=False)
    return df


if __name__ == "__main__":
    """
    Утилитарный запуск:
    python -m src.preprocessing
    """
    RAW = "data/raw/netflix_reviews.csv"
    BINARY = "data/processed/reviews_binary.csv"
    CLEAN = "data/processed/reviews_binary_clean.csv"
    THREECLASS = "data/processed/reviews_3class_clean.csv"
    # RAW = "netflix_reviews.csv"
    # BINARY = "reviews_binary.csv"
    # CLEAN = "reviews_binary_clean.csv"
    # THREECLASS = "reviews_3class_clean.csv"

    df_bin = make_binary_dataset(RAW, BINARY)
    df_clean = make_clean_dataset(BINARY, CLEAN)
    df_3cls = make_3class_dataset(CLEAN, THREECLASS)

    print("Binary:", df_bin.shape)
    print("Clean: ", df_clean.shape)
    print("3class:", df_3cls['sentiment_3class'].value_counts().to_dict())